{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Global Sensitivity Analysis Tutorial\n",
    "\n",
    "This tutorial provides an in-depth exploration of GSA capabilities in MCPost, covering advanced techniques and interpretation strategies.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding GSA Metrics](#understanding-gsa-metrics)\n",
    "2. [The Ishigami Function](#the-ishigami-function)\n",
    "3. [Advanced GSA Configuration](#advanced-gsa-configuration)\n",
    "4. [Interpreting Results](#interpreting-results)\n",
    "5. [Handling Real-World Challenges](#handling-real-world-challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mcpost import gsa_pipeline, gsa_for_target\n",
    "from mcpost import plot_sensitivity_metrics\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"MCPost comprehensive GSA tutorial loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding GSA Metrics\n",
    "\n",
    "MCPost provides multiple sensitivity metrics, each capturing different aspects of parameter influence:\n",
    "\n",
    "### Mutual Information (MI)\n",
    "- Measures statistical dependence between parameters and outputs\n",
    "- Captures both linear and nonlinear relationships\n",
    "- Range: [0, ∞), higher values indicate stronger influence\n",
    "- Model-agnostic and distribution-free\n",
    "\n",
    "### Distance Correlation (dCor)\n",
    "- Measures dependence between random vectors\n",
    "- Range: [0, 1], where 0 = independent, 1 = perfectly dependent\n",
    "- Captures nonlinear and non-monotonic relationships\n",
    "- Particularly useful for complex, multimodal relationships\n",
    "\n",
    "### Permutation Importance\n",
    "- Model-agnostic importance measure\n",
    "- Computed by shuffling parameter values and measuring performance decrease\n",
    "- Directly interpretable: how much does randomizing this parameter hurt predictions?\n",
    "- Robust to model assumptions\n",
    "\n",
    "### Sobol' Indices\n",
    "- **Si (First-order)**: Main effect of parameter i\n",
    "- **STi (Total)**: Total effect including all interactions involving parameter i\n",
    "- **Interaction effects**: STi - Si measures interaction contributions\n",
    "- Variance-based decomposition: Si + interactions = 1\n",
    "\n",
    "### Gaussian Process ARD\n",
    "- Automatic Relevance Determination from GP surrogate\n",
    "- 1/ARD_LS indicates relative parameter importance\n",
    "- Based on learned length scales in GP kernel\n",
    "- Reflects how \"relevant\" each parameter is for GP predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ishigami Function\n",
    "\n",
    "The Ishigami function is a standard benchmark for GSA methods:\n",
    "\n",
    "```\n",
    "f(x1, x2, x3) = sin(x1) + 7*sin(x2)^2 + 0.1*x3^4*sin(x1)\n",
    "```\n",
    "\n",
    "**Theoretical Sobol' indices:**\n",
    "- S1 = 0.314, S2 = 0.442, S3 = 0.000\n",
    "- ST1 = 0.558, ST2 = 0.442, ST3 = 0.244\n",
    "\n",
    "**Key insights:**\n",
    "- x2 has the largest main effect (S2 = 0.442)\n",
    "- x1 has moderate main effect but strong interactions (ST1 - S1 = 0.244)\n",
    "- x3 has no main effect but contributes through interactions with x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ishigami function\n",
    "def ishigami_function(X):\n",
    "    \"\"\"\n",
    "    Ishigami function: f(x1,x2,x3) = sin(x1) + 7*sin(x2)^2 + 0.1*x3^4*sin(x1)\n",
    "    \n",
    "    Parameters should be in range [-π, π]\n",
    "    \"\"\"\n",
    "    x1, x2, x3 = X[:, 0], X[:, 1], X[:, 2]\n",
    "    return np.sin(x1) + 7 * np.sin(x2)**2 + 0.1 * x3**4 * np.sin(x1)\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 5000\n",
    "X_ishigami = np.random.uniform(-np.pi, np.pi, (n_samples, 3))\n",
    "\n",
    "# Evaluate function\n",
    "y_ishigami = ishigami_function(X_ishigami)\n",
    "Y_ishigami = y_ishigami.reshape(-1, 1)\n",
    "\n",
    "print(f\"Generated {n_samples} Ishigami function samples\")\n",
    "print(f\"Output range: [{y_ishigami.min():.3f}, {y_ishigami.max():.3f}]\")\n",
    "print(f\"Output std: {y_ishigami.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive GSA on Ishigami function\n",
    "param_names = [\"x1\", \"x2\", \"x3\"]\n",
    "feature_names = [\"ishigami\"]\n",
    "\n",
    "print(\"Running comprehensive GSA on Ishigami function...\")\n",
    "results_ishigami = gsa_pipeline(\n",
    "    X_ishigami, Y_ishigami,\n",
    "    param_names=param_names,\n",
    "    feature_names=feature_names,\n",
    "    scaler=\"minmax\",\n",
    "    enable_sobol=True,\n",
    "    enable_gp=True,\n",
    "    enable_perm=True,\n",
    "    make_pdp=True,\n",
    "    topk_pdp=3,\n",
    "    N_sobol=8192  # Higher for better Sobol accuracy\n",
    ")\n",
    "\n",
    "# Display results\n",
    "ishigami_table = results_ishigami[\"results\"][\"ishigami\"][\"table\"]\n",
    "print(\"\\nIshigami Function - GSA Results:\")\n",
    "print(ishigami_table)\n",
    "\n",
    "# Compare with theoretical values\n",
    "theoretical = {\"x1\": {\"Si\": 0.314, \"STi\": 0.558}, \n",
    "               \"x2\": {\"Si\": 0.442, \"STi\": 0.442}, \n",
    "               \"x3\": {\"Si\": 0.000, \"STi\": 0.244}}\n",
    "\n",
    "print(\"\\nComparison with theoretical Sobol indices:\")\n",
    "print(f\"{'Param':<5} {'Si_computed':<12} {'Si_theory':<11} {'STi_computed':<13} {'STi_theory':<12}\")\n",
    "print(\"-\" * 65)\n",
    "for param in param_names:\n",
    "    si_comp = ishigami_table.loc[param, 'Si'] if 'Si' in ishigami_table.columns else 0\n",
    "    sti_comp = ishigami_table.loc[param, 'STi'] if 'STi' in ishigami_table.columns else 0\n",
    "    si_theory = theoretical[param]['Si']\n",
    "    sti_theory = theoretical[param]['STi']\n",
    "    print(f\"{param:<5} {si_comp:<12.3f} {si_theory:<11.3f} {sti_comp:<13.3f} {sti_theory:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity results\n",
    "fig, ax = plot_sensitivity_metrics(\n",
    "    ishigami_table,\n",
    "    title=\"Ishigami Function - Comprehensive Sensitivity Analysis\",\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- x2 shows highest sensitivity across all metrics (expected)\")\n",
    "print(\"- x1 shows moderate main effect but high total effect (interactions with x3)\")\n",
    "print(\"- x3 shows low main effect but contributes through interactions\")\n",
    "print(\"- Different metrics provide complementary insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced GSA Configuration\n",
    "\n",
    "MCPost provides many configuration options to customize your analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scaling options\n",
    "scaling_options = ['minmax', 'standard', None]\n",
    "scaling_results = {}\n",
    "\n",
    "print(\"Comparing scaling options on Ishigami function:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for scaler in scaling_options:\n",
    "    print(f\"\\nTesting scaler: {scaler}\")\n",
    "    \n",
    "    result = gsa_for_target(\n",
    "        X_ishigami, y_ishigami,\n",
    "        param_names=param_names,\n",
    "        scaler=scaler,\n",
    "        enable_perm=True,\n",
    "        enable_gp=True,\n",
    "        enable_sobol=True,\n",
    "        N_sobol=4096\n",
    "    )\n",
    "    \n",
    "    scaling_results[str(scaler)] = result\n",
    "    \n",
    "    # Show key metrics\n",
    "    print(f\"  MI ranking: {result.sort_values('MI', ascending=False).index.tolist()}\")\n",
    "    if 'Si' in result.columns:\n",
    "        print(f\"  Sobol Si ranking: {result.sort_values('Si', ascending=False).index.tolist()}\")\n",
    "\n",
    "print(\"\\nNote: Different scalers can affect GP-based metrics but shouldn't change rankings significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different kernel types for GP\n",
    "kernel_options = ['rbf', 'matern32', 'matern52']\n",
    "kernel_results = {}\n",
    "\n",
    "print(\"Comparing GP kernel options:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for kernel in kernel_options:\n",
    "    print(f\"\\nTesting kernel: {kernel}\")\n",
    "    \n",
    "    result = gsa_for_target(\n",
    "        X_ishigami, y_ishigami,\n",
    "        param_names=param_names,\n",
    "        scaler='minmax',\n",
    "        kernel_kind=kernel,\n",
    "        enable_gp=True,\n",
    "        enable_perm=False,  # Skip for speed\n",
    "        enable_sobol=False  # Skip for speed\n",
    "    )\n",
    "    \n",
    "    kernel_results[kernel] = result\n",
    "    \n",
    "    # Show GP-based metrics\n",
    "    if '1/ARD_LS' in result.columns:\n",
    "        ard_ranking = result.sort_values('1/ARD_LS', ascending=False).index.tolist()\n",
    "        print(f\"  ARD ranking: {ard_ranking}\")\n",
    "        print(f\"  ARD values: {result['1/ARD_LS'].round(3).to_dict()}\")\n",
    "\n",
    "print(\"\\nNote: Different kernels encode different smoothness assumptions\")\n",
    "print(\"- RBF: Infinitely smooth (most common)\")\n",
    "print(\"- Matérn 3/2: Less smooth, more flexible\")\n",
    "print(\"- Matérn 5/2: Moderately smooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results\n",
    "\n",
    "Understanding what different metrics tell you about your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function with known characteristics for interpretation\n",
    "def interpretation_function(X):\n",
    "    \"\"\"\n",
    "    Function designed to demonstrate different sensitivity patterns:\n",
    "    - x1: Linear effect\n",
    "    - x2: Quadratic effect  \n",
    "    - x3: Interaction with x1\n",
    "    - x4: Threshold effect\n",
    "    - x5: Noise parameter (minimal effect)\n",
    "    \"\"\"\n",
    "    x1, x2, x3, x4, x5 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "    \n",
    "    linear_term = 2 * x1\n",
    "    quadratic_term = x2**2\n",
    "    interaction_term = x1 * x3\n",
    "    threshold_term = np.where(x4 > 0, x4**2, 0)\n",
    "    noise_term = 0.01 * x5\n",
    "    \n",
    "    return linear_term + quadratic_term + interaction_term + threshold_term + noise_term\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 3000\n",
    "X_interp = np.random.uniform(-1, 1, (n_samples, 5))\n",
    "y_interp = interpretation_function(X_interp)\n",
    "\n",
    "param_names_interp = [\"x1_linear\", \"x2_quadratic\", \"x3_interaction\", \"x4_threshold\", \"x5_noise\"]\n",
    "\n",
    "print(\"Interpretation Function Analysis:\")\n",
    "print(f\"Generated {n_samples} samples\")\n",
    "print(f\"Expected ranking: x2 > x1 ≈ x3 > x4 > x5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GSA on interpretation function\n",
    "results_interp = gsa_for_target(\n",
    "    X_interp, y_interp,\n",
    "    param_names=param_names_interp,\n",
    "    scaler='minmax',\n",
    "    enable_perm=True,\n",
    "    enable_gp=True,\n",
    "    enable_sobol=True,\n",
    "    N_sobol=4096\n",
    ")\n",
    "\n",
    "print(\"\\nInterpretation Function - GSA Results:\")\n",
    "print(results_interp[['MI', 'dCor', 'PermMean', 'Si', 'STi']].round(4))\n",
    "\n",
    "# Analyze rankings\n",
    "metrics = ['MI', 'dCor', 'PermMean', 'Si', 'STi']\n",
    "print(\"\\nRankings by different metrics:\")\n",
    "for metric in metrics:\n",
    "    if metric in results_interp.columns:\n",
    "        ranking = results_interp.sort_values(metric, ascending=False).index.tolist()\n",
    "        print(f\"{metric:<12}: {ranking}\")\n",
    "\n",
    "print(\"\\nInterpretation insights:\")\n",
    "print(\"- x2_quadratic should rank high (strong nonlinear effect)\")\n",
    "print(\"- x1_linear should rank high (strong linear effect)\")\n",
    "print(\"- x3_interaction shows up in total effects (STi) more than main effects (Si)\")\n",
    "print(\"- x4_threshold may show different rankings depending on metric sensitivity to discontinuities\")\n",
    "print(\"- x5_noise should consistently rank lowest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Real-World Challenges\n",
    "\n",
    "Real-world GSA often involves challenges like correlated inputs, missing data, and computational constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Correlated inputs\n",
    "print(\"Challenge 1: Handling Correlated Inputs\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Generate correlated inputs\n",
    "n_samples = 2000\n",
    "mean = [0, 0, 0]\n",
    "cov = [[1.0, 0.7, 0.3],    # x1 strongly correlated with x2\n",
    "       [0.7, 1.0, 0.1],    # x2 moderately correlated with x3\n",
    "       [0.3, 0.1, 1.0]]    # x3 weakly correlated with others\n",
    "\n",
    "X_corr = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "# Simple linear model\n",
    "def correlated_model(X):\n",
    "    return 2*X[:, 0] + X[:, 1] + 0.5*X[:, 2]\n",
    "\n",
    "y_corr = correlated_model(X_corr)\n",
    "\n",
    "# Run GSA\n",
    "results_corr = gsa_for_target(\n",
    "    X_corr, y_corr,\n",
    "    param_names=[\"x1\", \"x2\", \"x3\"],\n",
    "    scaler='standard',  # Better for correlated normal variables\n",
    "    enable_perm=True,\n",
    "    enable_sobol=True\n",
    ")\n",
    "\n",
    "print(\"\\nCorrelated inputs - GSA results:\")\n",
    "print(results_corr[['MI', 'dCor', 'PermMean', 'Si']].round(4))\n",
    "\n",
    "# Check correlation matrix\n",
    "corr_matrix = np.corrcoef(X_corr.T)\n",
    "print(\"\\nInput correlation matrix:\")\n",
    "print(pd.DataFrame(corr_matrix, index=[\"x1\", \"x2\", \"x3\"], columns=[\"x1\", \"x2\", \"x3\"]).round(3))\n",
    "\n",
    "print(\"\\nNote: With correlated inputs, interpretation becomes more complex.\")\n",
    "print(\"Permutation importance may be more reliable than Sobol indices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Computational efficiency for large problems\n",
    "print(\"\\nChallenge 2: Computational Efficiency\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Simulate a larger problem\n",
    "n_params_large = 20\n",
    "n_samples_large = 5000\n",
    "\n",
    "print(f\"Large problem: {n_params_large} parameters, {n_samples_large} samples\")\n",
    "\n",
    "# Generate data\n",
    "X_large = np.random.uniform(-1, 1, (n_samples_large, n_params_large))\n",
    "\n",
    "# Simple additive model with decreasing importance\n",
    "def large_model(X):\n",
    "    weights = np.exp(-np.arange(X.shape[1]) * 0.3)  # Exponentially decreasing importance\n",
    "    return np.sum(X * weights, axis=1)\n",
    "\n",
    "y_large = large_model(X_large)\n",
    "param_names_large = [f\"param_{i+1}\" for i in range(n_params_large)]\n",
    "\n",
    "# Fast GSA configuration (skip expensive methods)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results_large = gsa_for_target(\n",
    "    X_large, y_large,\n",
    "    param_names=param_names_large,\n",
    "    scaler='minmax',\n",
    "    enable_perm=False,    # Skip permutation importance (expensive)\n",
    "    enable_gp=True,       # Keep GP (relatively fast)\n",
    "    enable_sobol=False,   # Skip Sobol (expensive for many parameters)\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFast GSA completed in {elapsed_time:.2f} seconds\")\n",
    "print(\"\\nTop 10 most important parameters:\")\n",
    "top_params = results_large.sort_values('MI', ascending=False).head(10)\n",
    "print(top_params[['MI', 'dCor']].round(4))\n",
    "\n",
    "print(\"\\nComputational tips:\")\n",
    "print(\"- Use MI and dCor for quick screening\")\n",
    "print(\"- Enable GP for moderate-sized problems (< 50 parameters)\")\n",
    "print(\"- Use Sobol indices only for final analysis of top parameters\")\n",
    "print(\"- Consider chunked processing for very large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This comprehensive tutorial covered:\n",
    "\n",
    "1. **Multiple GSA metrics** and their interpretations\n",
    "2. **Benchmark testing** with the Ishigami function\n",
    "3. **Configuration options** for different scenarios\n",
    "4. **Result interpretation** strategies\n",
    "5. **Real-world challenges** and solutions\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start with fast methods** (MI, dCor) for initial screening\n",
    "2. **Use multiple metrics** for robust conclusions\n",
    "3. **Consider input correlations** when interpreting results\n",
    "4. **Validate with known benchmarks** when possible\n",
    "5. **Choose appropriate scaling** based on your parameter distributions\n",
    "6. **Use sufficient sample sizes** (> 1000 for reliable results)\n",
    "\n",
    "### When to use each metric:\n",
    "\n",
    "- **MI & dCor**: General-purpose, fast, good for screening\n",
    "- **Permutation Importance**: When you have a good predictive model\n",
    "- **Sobol Indices**: For variance decomposition and interaction analysis\n",
    "- **GP ARD**: When you want to fit a surrogate model anyway\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Apply these techniques to your own models\n",
    "- Experiment with different configurations\n",
    "- Combine GSA with Monte Carlo integration for comprehensive analysis\n",
    "- Explore the MCPost extension system for custom methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}